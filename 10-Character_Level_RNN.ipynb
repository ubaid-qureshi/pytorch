{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary: </b>Building a classifier which gives the language of the given name \n",
    "\n",
    "<b>Datasets: </b>https://download.pytorch.org/tutorial/data.zip\n",
    "\n",
    "<b>Ref: </b>https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use glob library to load all the files in file_path directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =  'datasets/name_by_language/names'\n",
    "allFiles = glob.glob(file_path + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/name_by_language/names/Czech.txt',\n",
       " 'datasets/name_by_language/names/German.txt',\n",
       " 'datasets/name_by_language/names/Arabic.txt',\n",
       " 'datasets/name_by_language/names/Japanese.txt',\n",
       " 'datasets/name_by_language/names/Chinese.txt',\n",
       " 'datasets/name_by_language/names/Vietnamese.txt',\n",
       " 'datasets/name_by_language/names/French.txt',\n",
       " 'datasets/name_by_language/names/Irish.txt',\n",
       " 'datasets/name_by_language/names/Spanish.txt',\n",
       " 'datasets/name_by_language/names/Greek.txt',\n",
       " 'datasets/name_by_language/names/Italian.txt',\n",
       " 'datasets/name_by_language/names/Scottish.txt',\n",
       " 'datasets/name_by_language/names/Dutch.txt',\n",
       " 'datasets/name_by_language/names/Korean.txt',\n",
       " 'datasets/name_by_language/names/Polish.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = pd.DataFrame()\n",
    "list_ = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will read every file turn by turn\n",
    "* data in txt files are not coma separated, there is new name in every line<br>\n",
    "    Hence we will use sep = '/n', error_bad_lines will avoid all the lines which are not separated properly in dataset\n",
    "* we will use **split** function to form targets of all names\n",
    "    We will basically extract language name from filename\n",
    "* We will have DF for every language, now we will murge them in one DF\n",
    "    We will do this by first making a list of dfs and then concatanate all dfs in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,\n",
    "                     sep='/n',\n",
    "                     names = ['Name'],\n",
    "                     engine=\"python\",\n",
    "                     error_bad_lines=False)\n",
    "    \n",
    "    df['language'] = str(file_.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    list_.append(df)\n",
    "    names_data = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Jeong</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Fenyo</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Guo</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Peerenboom</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>You</td>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Gallego</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Kurzmann</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Muhlfeld</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Rocha</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Agani</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name language\n",
       "29        Jeong   Korean\n",
       "83        Fenyo    Czech\n",
       "47          Guo  Chinese\n",
       "173  Peerenboom    Dutch\n",
       "89          You   Korean\n",
       "113     Gallego  Spanish\n",
       "374    Kurzmann   German\n",
       "449    Muhlfeld   German\n",
       "221       Rocha  Spanish\n",
       "52        Agani  Italian"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = names_data['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese',\n",
       "       'French', 'Irish', 'Spanish', 'Greek', 'Italian', 'Scottish',\n",
       "       'Dutch', 'Korean', 'Polish'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the duplicate names\n",
    "The drop_duplicates() function in pandas allows removal of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = names_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4931"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all posible letters\n",
    "This will allow us to create a one-hot-encoded tensor for the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "all_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert name in tensors \n",
    "This effectively performs one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def name_to_tensor(name):\n",
    "    name_in_tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for i, letter in enumerate(name):\n",
    "        name_in_tensor[i][0][all_letters.find(letter)] = 1\n",
    "        \n",
    "    return name_in_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check what names may look like when converted to tensors\n",
    "You can see a(small) is first element and A(capital) is somewhere in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_tensor('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "           0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_tensor('a A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the RNN\n",
    "Explanation of the model below\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network\n",
    "\n",
    "To run a step of this network we need to pass an input (in our case, the Tensor for the current letter) and a previous hidden state (which we initialize as zeros at first). We’ll get back the output (probability of each language) and a next hidden state (which we keep for the next step).\n",
    "\n",
    "* the <b>i2h</b> layer is an input-to-hidden layer while <b>i2o</b> is input-to-output <br /> \n",
    "* the <b>combined</b> layer performs the combination of the current input letter along with the value of the previous hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)    \n",
    "        output = self.i2o(combined)    \n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an RNN\n",
    "We specify the number of hidden layers along with the output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "n_languages= len(languages)\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, output_size = n_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters for training the model\n",
    "* we perform 100,000 iterations which will ensure a thorough coverage of the 10,000 names in the dataset\n",
    "* the loss function is negative log likelihood loss\n",
    "* we initialize a learning rate of 0.005 which will decrease with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100000\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert a prediction to the string label for language\n",
    "We get a list of probabilities for each language and then from index of each language we get the name of language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_language (output):\n",
    "    \n",
    "    top_n, top_index = output.topk(1)\n",
    "    pred_i = top_index[0].item()\n",
    "    pred = languages[pred_i] \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the random module\n",
    "We will be picking names randomly from our dataset for which we will use the random module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the training \n",
    "* we pick a name randomly from the dataset and convert it to a tensor\n",
    "* we get the actual label for that name\n",
    "* the RNN is initialized with zero gradients\n",
    "* for each character in the name:\n",
    " * we use the RNN to perform a prediction on letters of the name up to that character\n",
    "* we calculate the loss based on the predicted and actual values of language\n",
    "* we perform a back propagation to recalibrate the weights in the NN\n",
    "* we update the parameters of the NN by adding to them their gradient and subtracting the learning rate (to slow down the learning)\n",
    "\n",
    "Finally, for every 5000th iteration, we print out the name, the prediction and the actual label along with the calculated loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters- 5000 5% (3.724365711212158) Name- Macleod Language- German ✗ (Scottish)\n",
      "iters- 10000 10% (2.8470005989074707) Name- Favager Language- German ✗ (French)\n",
      "iters- 15000 15% (1.484479308128357) Name- Rosales Language- Spanish ✓\n",
      "iters- 20000 20% (1.1251862049102783) Name- Brune Language- German ✓\n",
      "iters- 25000 25% (0.0027055158279836178) Name- Tsukamoto Language- Japanese ✓\n",
      "iters- 30000 30% (0.25187188386917114) Name- Ansaldi Language- Italian ✓\n",
      "iters- 35000 35% (0.10041199624538422) Name- Mazuka Language- Japanese ✓\n",
      "iters- 40000 40% (0.06594379246234894) Name- Alexandropoulos Language- Greek ✓\n",
      "iters- 45000 45% (1.5990803241729736) Name- Blanco Language- Italian ✗ (Spanish)\n",
      "iters- 50000 50% (1.411132574081421) Name- Abelló Language- Spanish ✗ (Italian)\n",
      "iters- 55000 55% (0.7573774456977844) Name- Leitz Language- German ✓\n",
      "iters- 60000 60% (0.34390515089035034) Name- Di caprio Language- Italian ✓\n",
      "iters- 65000 65% (1.3341048955917358) Name- Cearbhall Language- German ✗ (Irish)\n",
      "iters- 70000 70% (0.08459968864917755) Name- Karameros Language- Greek ✓\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1 , iterations +1):\n",
    "\n",
    "    i = random.randint(0, len(names_data) - 1)\n",
    "    \n",
    "    name = names_data.iloc[i][0]\n",
    "    name_in_tensor = name_to_tensor(name)\n",
    "    \n",
    "    language = names_data.iloc[i][1]\n",
    "    language_in_tensor = torch.tensor([list(languages).index(language)], dtype=torch.long)\n",
    "    \n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(name_in_tensor.size()[0]):\n",
    "        output, hidden = rnn(name_in_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, language_in_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    current_loss += loss.item()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    if iteration% 5000 == 0:\n",
    "        \n",
    "        pred = output_to_language(output)\n",
    "        \n",
    "        correct = '✓' if pred == language else '✗ (%s)' % language\n",
    "        print('iters- %d %d%% (%s) Name- %s Language- %s %s' % \\\n",
    "              (iteration, iteration/iterations*100, loss.item(), name, pred, correct))\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        all_losses.append(current_loss / 1000)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a test using 10,000 randomly selected names\n",
    "We peform steps similar to what was done in training. \n",
    "* pick a name at random, conver to tensor\n",
    "* convert the label to a tensor\n",
    "* initialize the RNN's hidden layers\n",
    "* make a prediction for each additional character of the name and then get the final prediction after all the characters have been fed in\n",
    "\n",
    "After this, we create lists with the real and predicted values of language. We will use these to plot a confusion matrix to check the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_confusion = 10000\n",
    "\n",
    "prediction=[]\n",
    "actual = []\n",
    "\n",
    "for _ in range(n_confusion):\n",
    "\n",
    "    i = random.randint(0, len(names_data) - 1)\n",
    "    \n",
    "    name = names_data.iloc[i][0]\n",
    "    name_in_tensor = name_to_tensor(name)\n",
    "    \n",
    "    language = names_data.iloc[i][1]\n",
    "    language_in_tensor = torch.tensor([list(languages).index(language)], dtype=torch.long)\n",
    "        \n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for j in range(name_in_tensor.size()[0]):\n",
    "        output, hidden = rnn(name_in_tensor[j], hidden)\n",
    "    \n",
    "    pred = output_to_language(output)\n",
    "    \n",
    "    prediction.append(pred)\n",
    "    actual.append(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install pandas_ml\n",
    "This is needed for the confusion matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pandas_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual,prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = ConfusionMatrix(actual, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i, data in enumerate(actual):\n",
    "    if data == prediction[i]:\n",
    "        correct += 1\n",
    "    \n",
    "print('Accuracy of this language classifier is ', correct/n_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
